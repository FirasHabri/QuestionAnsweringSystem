{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Firas H\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('intents.json', encoding=\"utf-8\") as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 documents\n",
      "5 classes ['DateOfBirth', 'club', 'height', 'position', 'region']\n",
      "21 unique stemmed words ['ادش', 'اعب', 'اي', 'ايم', 'اين', 'حفظ', 'دين', 'ركز', 'شو', 'طول', 'عاش', 'عمر', 'لعب', 'ما', 'من', 'ندي', 'هو', 'هي', 'ولد', 'وين', 'يلد']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X :\n",
      " [[0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "Train Y :\n",
      " [[0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X :\\n\",train_x[:5])\n",
    "print(\"\\nTrain Y :\\n\",train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50),solver='sgd',learning_rate_init=0.01,max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=50, learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.50\n",
    "def classify(sentence):\n",
    "    results = mlp.predict([bow(sentence, words)])[0]\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    return return_list\n",
    "\n",
    "def response(sentence):\n",
    "    results = classify(sentence)\n",
    "    if results:\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    print(i['tag'])\n",
    "\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "height\n",
      "[('height', 1)]\n",
      "------------\n",
      "region\n",
      "[('region', 1)]\n",
      "------------\n",
      "club\n",
      "[('club', 1), ('position', 1)]\n"
     ]
    }
   ],
   "source": [
    "res = [\n",
    "    \"شئد طول احمد الصالح ؟\",\n",
    "    \"خالد حاج عثمان وين ولد ؟\",\n",
    "    \"اشو مركز حسين جنيد و بأي نادي بيلعب ؟\"\n",
    "]\n",
    "for r in res:\n",
    "    print(\"------------\")\n",
    "    print(response(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_excel('players.xlsx', encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>إسم الاعب</th>\n",
       "      <th>الميلاد</th>\n",
       "      <th>المحافظة</th>\n",
       "      <th>الطول</th>\n",
       "      <th>مركز اللعب</th>\n",
       "      <th>النادي</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>إبراهيم عالمة</td>\n",
       "      <td>18-10-1991</td>\n",
       "      <td>حمص</td>\n",
       "      <td>185</td>\n",
       "      <td>حارس مرمى</td>\n",
       "      <td>الوحدة السوري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>احمد مدنية</td>\n",
       "      <td>1-1-1990</td>\n",
       "      <td>الاذقية</td>\n",
       "      <td>177</td>\n",
       "      <td>حارس مرمى</td>\n",
       "      <td>الجيش السوري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>خالد حاج عثمان</td>\n",
       "      <td>1-5-1987</td>\n",
       "      <td>حلب</td>\n",
       "      <td>185</td>\n",
       "      <td>حارس مرمى</td>\n",
       "      <td>ضمك السعودي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>احمد الصالح</td>\n",
       "      <td>20-5-1998</td>\n",
       "      <td>القامشلي</td>\n",
       "      <td>183</td>\n",
       "      <td>مدافع</td>\n",
       "      <td>العهد اللبناني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حسين جويد</td>\n",
       "      <td>1-1-1993</td>\n",
       "      <td>حلب</td>\n",
       "      <td>178</td>\n",
       "      <td>مدافع</td>\n",
       "      <td>الزوراء العراقي</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        إسم الاعب     الميلاد  المحافظة  الطول مركز اللعب           النادي\n",
       "0   إبراهيم عالمة  18-10-1991       حمص    185  حارس مرمى    الوحدة السوري\n",
       "1      احمد مدنية    1-1-1990   الاذقية    177  حارس مرمى     الجيش السوري\n",
       "2  خالد حاج عثمان    1-5-1987       حلب    185  حارس مرمى      ضمك السعودي\n",
       "3     احمد الصالح   20-5-1998  القامشلي    183      مدافع   العهد اللبناني\n",
       "4       حسين جويد    1-1-1993       حلب    178      مدافع  الزوراء العراقي"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5410: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كرة</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>القدم</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رياضة</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>جماعية</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>،</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>يضم</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>فريق</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>كرة</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>القدم</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word Tag\n",
       "0     كرة   O\n",
       "1   القدم   O\n",
       "2   رياضة   O\n",
       "3  جماعية   O\n",
       "4       ،   O\n",
       "5     يضم   O\n",
       "6    فريق   O\n",
       "7     كرة   O\n",
       "8   القدم   O\n",
       "9      11   O"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Football.txt', encoding = \"UTF-8\", sep=\",\", error_bad_lines=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.Tag)):\n",
    "    df.Tag[i] = df.Tag[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "train_arr = []\n",
    "test_arr = []\n",
    "train_lbl = []\n",
    "test_lbl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr=train['Word'].astype(str)\n",
    "train_lbl=train['Tag'].astype(str)\n",
    "test_arr=test['Word'].astype(str)\n",
    "test_lbl=test['Tag'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels :\n",
      " 4056    O\n",
      "3247    O\n",
      "3259    O\n",
      "2588    O\n",
      "86      O\n",
      "Name: Tag, dtype: object\n",
      "\n",
      "Train Words :\n",
      " 4056          ،\n",
      "3247          و\n",
      "3259    المنتخب\n",
      "2588       أنها\n",
      "86      الرياضة\n",
      "Name: Word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels :\\n\",train_lbl[15:20])\n",
    "print(\"\\nTrain Words :\\n\",train_arr[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_arr)\n",
    "train_mat = vectorizer.transform(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(train_mat)\n",
    "train_tfmat = tfidf.transform(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mat = vectorizer.transform(test_arr)\n",
    "test_tfmat = tfidf.transform(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4536x1532 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4027 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvm=LinearSVC()\n",
    "lsvm.fit(train_tfmat,train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lsvm=lsvm.predict(test_tfmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-reg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['حلب']\n",
    "test_str = vectorizer.transform(test)\n",
    "test_tfstr = tfidf.transform(test_str)\n",
    "test_tfstr.shape\n",
    "lsvm.predict(test_tfstr.toarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9973568281938326\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", metrics.accuracy_score(test_lbl, y_pred_lsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اشو', 'مركز', 'إبراهيم', 'عالمة', 'و', 'بأي', 'نادي', 'بيلعب', '؟']\n"
     ]
    }
   ],
   "source": [
    "phrase=\"اشو مركز إبراهيم عالمة و بأي نادي بيلعب ؟\"\n",
    "arr=phrase.split()\n",
    "print(arr)\n",
    "\n",
    "import sys\n",
    "y=[]\n",
    "token=[]\n",
    "for x in arr:\n",
    "    x=[x]\n",
    "    test_str = vectorizer.transform(x)\n",
    "    test_tfstr = tfidf.transform(test_str)\n",
    "    test_tfstr.shape\n",
    "    token.append(x)\n",
    "    y.append(lsvm.predict(test_tfstr.toarray())[0])\n",
    "\n",
    "output =pd.DataFrame(list(zip(token,y)),columns=['token','entity_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>entity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[اشو]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[مركز]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[إبراهيم]</td>\n",
       "      <td>B-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[عالمة]</td>\n",
       "      <td>I-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[و]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[بأي]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[نادي]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[بيلعب]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[؟]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token entity_type\n",
       "0      [اشو]           O\n",
       "1     [مركز]           O\n",
       "2  [إبراهيم]      B-name\n",
       "3    [عالمة]      I-name\n",
       "4        [و]           O\n",
       "5      [بأي]           O\n",
       "6     [نادي]           O\n",
       "7    [بيلعب]           O\n",
       "8        [؟]           O"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ''\n",
    "org = ''\n",
    "reg = ''\n",
    "pos = ''\n",
    "for i in range(len(output.loc[:,'entity_type'])):\n",
    "    if output.loc[i,'entity_type'] == 'B-name' or output.loc[i,'entity_type']=='I-name':\n",
    "        name = name + output.loc[i,'token'][0] + ' '\n",
    "for i in range(len(output.loc[:,'entity_type'])):\n",
    "    if output.loc[i,'entity_type'] == 'B-org' or output.loc[i,'entity_type']=='I-org':\n",
    "        org = org + output.loc[i,'token'][0] + ' '\n",
    "for i in range(len(output.loc[:,'entity_type'])):\n",
    "    if output.loc[i,'entity_type'] == 'B-reg' or output.loc[i,'entity_type']=='I-reg':\n",
    "        reg = reg + output.loc[i,'token'][0] + ' '\n",
    "for i in range(len(output.loc[:,'entity_type'])):\n",
    "    if output.loc[i,'entity_type'] == 'B-pos' or output.loc[i,'entity_type']=='I-pos':\n",
    "        pos = pos + output.loc[i,'token'][0] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer(players,intent):\n",
    "    temp = players\n",
    "    ans = []\n",
    "    if name:\n",
    "        temp = temp.set_index('إسم الاعب')\n",
    "        ans.append(temp.loc[name.strip(),intent])\n",
    "        return ans\n",
    "    if org:\n",
    "        heights = temp[temp['النادي'] == org.strip()].loc[:,intent]\n",
    "        names = temp[temp['النادي'] == org.strip()].loc[:,'إسم الاعب']\n",
    "        for n, h in zip(names, heights):\n",
    "            ans.append(n + \" \" + str(h))\n",
    "        return ans\n",
    "    if reg:\n",
    "        heights = temp[temp['المحافظة'] == reg.strip()].loc[:,intent]\n",
    "        names = temp[temp['المحافظة'] == reg.strip()].loc[:,'إسم الاعب']\n",
    "        for n, h in zip(names, heights):\n",
    "            ans.append(n + \" \" + str(h))\n",
    "        return ans\n",
    "    if pos: \n",
    "        poss = pos.replace(\"ال\", \"\")\n",
    "        heights = temp[temp['مركز اللعب'] == poss.strip()].loc[:,intent]\n",
    "        names = temp[temp['مركز اللعب'] == poss.strip()].loc[:,'إسم الاعب']\n",
    "        for n, h in zip(names, heights):\n",
    "            ans.append(n + \" \" + str(h))\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "club\n",
      "[['الوحدة السوري'], ['حارس مرمى']]\n"
     ]
    }
   ],
   "source": [
    "intentDict = {\n",
    "    'height':'الطول',\n",
    "    'region':'المحافظة',\n",
    "    'club':'النادي',\n",
    "    'DateOfBirth':'الميلاد',\n",
    "    'position':'مركز اللعب'\n",
    "}\n",
    "ans = []\n",
    "intent = response(phrase)\n",
    "for i in range(len(intent)):\n",
    "    ans.append(Answer(players,intentDict[intent[i][0]]))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Generator with CFG rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.parse.generate\n",
    "import itertools\n",
    "import sys\n",
    "from nltk.grammar import Nonterminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammar, start=None, depth=None, n=None):\n",
    "    \"\"\"\n",
    "    Generates an iterator of all sentences from a CFG.\n",
    "\n",
    "    :param grammar: The Grammar used to generate sentences.\n",
    "    :param start: The Nonterminal from which to start generate sentences.\n",
    "    :param depth: The maximal depth of the generated tree.\n",
    "    :param n: The maximum number of sentences to return.\n",
    "    :return: An iterator of lists of terminal tokens.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = sys.maxsize\n",
    "\n",
    "    iter = _generate_all(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iter = itertools.islice(iter, n)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n",
    "\n",
    "def _generate_all(grammar, items, depth):\n",
    "    if items:\n",
    "        try:\n",
    "            for frag1 in _generate_one(grammar, items[0], depth):\n",
    "                for frag2 in _generate_all(grammar, items[1:], depth):\n",
    "                    yield frag1 + frag2\n",
    "        except RuntimeError as _error:\n",
    "            if _error.message == \"maximum recursion depth exceeded\":\n",
    "                # Helpful error message while still showing the recursion stack.\n",
    "                raise RuntimeError(\n",
    "                    \"The grammar has rule(s) that yield infinite recursion!!\"\n",
    "                )\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            for prod in grammar.productions(lhs=item):\n",
    "                for frag in _generate_all(grammar, prod.rhs(), depth - 1):\n",
    "                    yield frag\n",
    "        else:\n",
    "            yield [item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (MU مكان ولادة (NS اللاعب فلان الفلاني)) (KH (D هو تموضع))))\n"
     ]
    }
   ],
   "source": [
    "demo_grammar = \"\"\"\n",
    "  S -> NP\n",
    "  NP -> MU KH\n",
    "  MU -> 'مكان' 'ولادة' NS | 'طول' NS | 'نادي' NS | 'مركز' NS | 'تاريخ' 'ميلاد' NS\n",
    "  NS -> 'اللاعب' 'فلان' 'الفلاني'\n",
    "  KH ->  D\n",
    "  D -> 'هو' 'تموضع'\n",
    "\"\"\"\n",
    "grammar = nltk.CFG.fromstring(demo_grammar)\n",
    "sentence = \"مكان ولادة اللاعب فلان الفلاني هو تموضع\".split()\n",
    "def parse(sent):\n",
    "    #Returns nltk.Tree.Tree format output\n",
    "    a = []  \n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    for tree in parser.parse(sent):\n",
    "        a.append(tree)\n",
    "    return(a[0]) \n",
    "print(parse(sentence))\n",
    "parse(sentence).draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentences(N=10):\n",
    "    from nltk.grammar import CFG\n",
    "    responses = []\n",
    "    print('Generating the sentences for demo grammar:')\n",
    "    print(demo_grammar)\n",
    "    grammar = CFG.fromstring(demo_grammar)\n",
    "    for n, sent in enumerate(generate(grammar, n=10), 1):\n",
    "        responses.append(' '.join(sent))\n",
    "        print('%3d. %s' % (n, ' '.join(sent)))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the sentences for demo grammar:\n",
      "\n",
      "  S -> NP\n",
      "  NP -> MU KH\n",
      "  MU -> 'مكان' 'ولادة' NS | 'طول' NS | 'نادي' NS | 'مركز' NS | 'تاريخ' 'ميلاد' NS\n",
      "  NS -> 'اللاعب' 'فلان' 'الفلاني'\n",
      "  KH ->  D\n",
      "  D -> 'هو' 'تموضع'\n",
      "\n",
      "  1. مكان ولادة اللاعب فلان الفلاني هو تموضع\n",
      "  2. طول اللاعب فلان الفلاني هو تموضع\n",
      "  3. نادي اللاعب فلان الفلاني هو تموضع\n",
      "  4. مركز اللاعب فلان الفلاني هو تموضع\n",
      "  5. تاريخ ميلاد اللاعب فلان الفلاني هو تموضع\n"
     ]
    }
   ],
   "source": [
    "responses = generateSentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsesDict = {\n",
    "    'region' : responses[0],\n",
    "    'height' : responses[1],\n",
    "    'club' : responses[2],\n",
    "    'position' : responses[3],\n",
    "    'DateOfBirth' : responses[4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نادي اللاعب إبراهيم عالمة  هو الوحدة السوري\n",
      "مركز اللاعب إبراهيم عالمة  هو حارس مرمى\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(intent)):\n",
    "    s = responsesDict[intent[i][0]]\n",
    "    s = s.replace(\"تموضع\",ans[i][0])\n",
    "    s = s.replace(\"فلان الفلاني\",name)\n",
    "    print(s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
